# BUD - å¯ç”¨æ€§è¨­è¨ˆãƒ»é‹ç”¨ã‚¬ã‚¤ãƒ‰

BUD ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å¯ç”¨æ€§è¦ä»¶ã€å†—é•·åŒ–è¨­è¨ˆã€éšœå®³å¯¾å¿œã€ç½å®³å¾©æ—§è¨ˆç”»ã‚’å®šç¾©ã—ã¾ã™ã€‚

## ğŸ¯ å¯ç”¨æ€§ç›®æ¨™ãƒ»SLA å®šç¾©

### ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ç›®æ¨™ï¼ˆSLOï¼‰

| æŒ‡æ¨™                         | ç›®æ¨™å€¤         | æ¸¬å®šæœŸé–“ | é‡è¦åº¦ |
| ---------------------------- | -------------- | -------- | ------ |
| **ç¨¼åƒç‡ (Uptime)**          | 99.5%          | æœˆæ¬¡     | æœ€é«˜   |
| **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**           | 95%ãŒ 2 ç§’ä»¥å†… | æ—¥æ¬¡     | é«˜     |
| **API å¯ç”¨æ€§**               | 99.9%          | æœˆæ¬¡     | æœ€é«˜   |
| **éŸ³å£°å‡¦ç†å¯ç”¨æ€§**           | 99.0%          | æœˆæ¬¡     | é«˜     |
| **ãƒ‡ãƒ¼ã‚¿å¾©æ—§æ™‚é–“ (RTO)**     | < 4 æ™‚é–“       | éšœå®³æ™‚   | é«˜     |
| **ãƒ‡ãƒ¼ã‚¿æå¤±è¨±å®¹æ™‚é–“ (RPO)** | < 1 æ™‚é–“       | éšœå®³æ™‚   | æœ€é«˜   |

### SLA (Service Level Agreement)

```yaml
# ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«åˆæ„
monthly_uptime:
  target: 99.5%
  penalty:
    - threshold: 99.0%
      action: "ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ 5%"
    - threshold: 98.0%
      action: "ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ 10%"

response_time:
  target: "95% of requests < 2s"
  measurement: "monthly average"

planned_maintenance:
  max_duration: "4 hours/month"
  notification_period: "48 hours advance"
  allowed_window: "02:00-06:00 JST"
```

---

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ»å†—é•·åŒ–

### ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆè¨­è¨ˆ

#### 1. ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ§‹æˆ

```yaml
# infrastructure/regions.yml
regions:
  primary:
    name: "ap-northeast-1" # æ±äº¬
    services:
      - frontend (Next.js)
      - backend (FastAPI)
      - database (PostgreSQL Primary)
      - redis (Primary)

  secondary:
    name: "ap-southeast-1" # ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«
    services:
      - backend (FastAPI) - Read Replica
      - database (PostgreSQL Read Replica)
      - redis (Replica)

  backup:
    name: "us-west-2" # ã‚ªãƒ¬ã‚´ãƒ³
    services:
      - database (Backup)
      - file_storage (Backup)

availability_zones:
  primary_region:
    - ap-northeast-1a
    - ap-northeast-1c
    - ap-northeast-1d

  distribution_strategy: "multi-az"
  min_availability_zones: 2
```

#### 2. ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼è¨­è¨ˆ

```yaml
# infrastructure/load-balancer.yml
load_balancer:
  type: "Application Load Balancer"
  distribution: "round_robin"
  health_checks:
    interval: 30s
    timeout: 5s
    healthy_threshold: 2
    unhealthy_threshold: 3
    path: "/health"

  targets:
    frontend:
      - instance_type: "t3.medium"
      - min_capacity: 2
      - max_capacity: 10
      - auto_scaling: true

    backend:
      - instance_type: "t3.large"
      - min_capacity: 2
      - max_capacity: 8
      - auto_scaling: true

auto_scaling:
  metrics:
    - name: "CPUUtilization"
      target: 70%
    - name: "RequestCountPerTarget"
      target: 1000

  scale_out_cooldown: 300s
  scale_in_cooldown: 300s
```

#### 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†—é•·åŒ–

```python
# config/database.py
import asyncio
from sqlalchemy import create_engine
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

class DatabaseCluster:
    def __init__(self):
        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆèª­ã¿æ›¸ãï¼‰
        self.primary_engine = create_async_engine(
            "postgresql+asyncpg://user:pass@primary-db:5432/bud",
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )

        # ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
        self.replica_engines = [
            create_async_engine(
                "postgresql+asyncpg://user:pass@replica1-db:5432/bud",
                pool_size=15,
                max_overflow=20,
                pool_pre_ping=True
            ),
            create_async_engine(
                "postgresql+asyncpg://user:pass@replica2-db:5432/bud",
                pool_size=15,
                max_overflow=20,
                pool_pre_ping=True
            )
        ]

        self.current_replica_index = 0

    async def get_write_session(self) -> AsyncSession:
        """æ›¸ãè¾¼ã¿ç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒªDBï¼‰"""
        async_session = sessionmaker(
            self.primary_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        return async_session()

    async def get_read_session(self) -> AsyncSession:
        """èª­ã¿å–ã‚Šç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ï¼‰"""
        # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ã§ãƒ¬ãƒ—ãƒªã‚«ã‚’é¸æŠ
        replica_engine = await self._get_healthy_replica()

        async_session = sessionmaker(
            replica_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )
        return async_session()

    async def _get_healthy_replica(self):
        """å¥å…¨ãªãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ã‚’å–å¾—"""
        for i in range(len(self.replica_engines)):
            replica_index = (self.current_replica_index + i) % len(self.replica_engines)
            replica_engine = self.replica_engines[replica_index]

            try:
                # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
                async with replica_engine.connect() as conn:
                    await conn.execute("SELECT 1")

                self.current_replica_index = replica_index
                return replica_engine

            except Exception as e:
                logger.warn(f"ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«{replica_index}ãŒåˆ©ç”¨ä¸å¯",
                    error=str(e),
                    action="replica_health_check_failed"
                )
                continue

        # å…¨ãƒ¬ãƒ—ãƒªã‚«ãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã¯ãƒ—ãƒ©ã‚¤ãƒãƒªã‚’ä½¿ç”¨
        logger.warn("å…¨ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ãŒåˆ©ç”¨ä¸å¯ã€ãƒ—ãƒ©ã‚¤ãƒãƒªDBã‚’ä½¿ç”¨",
            action="fallback_to_primary"
        )
        return self.primary_engine

# ä¾å­˜æ€§æ³¨å…¥
db_cluster = DatabaseCluster()

# ã‚µãƒ¼ãƒ“ã‚¹ã§ã®ä½¿ç”¨ä¾‹
class ConversationService:
    async def get_conversations(self, user_id: int):
        """èª­ã¿å–ã‚Šå°‚ç”¨æ“ä½œï¼ˆãƒ¬ãƒ—ãƒªã‚«ä½¿ç”¨ï¼‰"""
        async with await db_cluster.get_read_session() as session:
            result = await session.execute(
                select(Conversation).where(Conversation.user_id == user_id)
            )
            return result.scalars().all()

    async def create_conversation(self, user_id: int, title: str):
        """æ›¸ãè¾¼ã¿æ“ä½œï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒªä½¿ç”¨ï¼‰"""
        async with await db_cluster.get_write_session() as session:
            conversation = Conversation(user_id=user_id, title=title)
            session.add(conversation)
            await session.commit()
            return conversation
```

---

## ğŸ”„ éšœå®³æ¤œçŸ¥ãƒ»è‡ªå‹•å¾©æ—§

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè£…

#### 1. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```python
# utils/health_check.py
from fastapi import FastAPI, HTTPException
from sqlalchemy import text
from datetime import datetime
from typing import Dict, Any
import asyncio
import httpx

class HealthChecker:
    def __init__(self, db_cluster, redis_client):
        self.db_cluster = db_cluster
        self.redis_client = redis_client
        self.external_services = {
            'openai': 'https://api.openai.com/v1/models',
            'firebase': 'https://firebase.googleapis.com/v1/projects'
        }

    async def comprehensive_health_check(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        checks = await asyncio.gather(
            self._check_database(),
            self._check_redis(),
            self._check_external_services(),
            self._check_file_system(),
            self._check_memory_usage(),
            return_exceptions=True
        )

        results = {
            'timestamp': datetime.utcnow().isoformat(),
            'overall_status': 'healthy',
            'checks': {
                'database': checks[0],
                'redis': checks[1],
                'external_services': checks[2],
                'file_system': checks[3],
                'memory': checks[4]
            }
        }

        # å…¨ä½“ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if any(isinstance(check, Exception) or
               (isinstance(check, dict) and check.get('status') == 'unhealthy')
               for check in checks):
            results['overall_status'] = 'unhealthy'

        return results

    async def _check_database(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª"""
        try:
            start_time = asyncio.get_event_loop().time()

            # ãƒ—ãƒ©ã‚¤ãƒãƒªDBç¢ºèª
            async with await self.db_cluster.get_write_session() as session:
                await session.execute(text("SELECT 1"))

            # ãƒªãƒ¼ãƒ‰ãƒ¬ãƒ—ãƒªã‚«ç¢ºèª
            async with await self.db_cluster.get_read_session() as session:
                await session.execute(text("SELECT 1"))

            response_time = (asyncio.get_event_loop().time() - start_time) * 1000

            return {
                'status': 'healthy',
                'response_time_ms': round(response_time, 2),
                'primary_db': 'connected',
                'read_replica': 'connected'
            }

        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'primary_db': 'error',
                'read_replica': 'unknown'
            }

    async def _check_redis(self) -> Dict[str, Any]:
        """Redisæ¥ç¶šç¢ºèª"""
        try:
            start_time = asyncio.get_event_loop().time()

            # æ›¸ãè¾¼ã¿ãƒ»èª­ã¿å–ã‚Šãƒ†ã‚¹ãƒˆ
            test_key = f"health_check_{int(start_time)}"
            await self.redis_client.set(test_key, "ok", ex=60)
            result = await self.redis_client.get(test_key)
            await self.redis_client.delete(test_key)

            response_time = (asyncio.get_event_loop().time() - start_time) * 1000

            if result == "ok":
                return {
                    'status': 'healthy',
                    'response_time_ms': round(response_time, 2),
                    'read_write': 'ok'
                }
            else:
                return {
                    'status': 'unhealthy',
                    'error': 'Redis read/write test failed'
                }

        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e)
            }

    async def _check_external_services(self) -> Dict[str, Any]:
        """å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹æ¥ç¶šç¢ºèª"""
        results = {}

        async with httpx.AsyncClient(timeout=10.0) as client:
            for service_name, url in self.external_services.items():
                try:
                    start_time = asyncio.get_event_loop().time()
                    response = await client.get(url)
                    response_time = (asyncio.get_event_loop().time() - start_time) * 1000

                    results[service_name] = {
                        'status': 'healthy' if response.status_code < 500 else 'degraded',
                        'status_code': response.status_code,
                        'response_time_ms': round(response_time, 2)
                    }

                except Exception as e:
                    results[service_name] = {
                        'status': 'unhealthy',
                        'error': str(e)
                    }

        overall_status = 'healthy'
        if any(result['status'] == 'unhealthy' for result in results.values()):
            overall_status = 'degraded'

        return {
            'status': overall_status,
            'services': results
        }

# FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
app = FastAPI()
health_checker = HealthChecker(db_cluster, redis_client)

@app.get("/health")
async def health_check():
    """ç°¡æ˜“ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "ok", "timestamp": datetime.utcnow().isoformat()}

@app.get("/health/detailed")
async def detailed_health_check():
    """è©³ç´°ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    result = await health_checker.comprehensive_health_check()

    if result['overall_status'] == 'unhealthy':
        raise HTTPException(status_code=503, detail=result)

    return result
```

#### 2. è‡ªå‹•å¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

```python
# utils/auto_recovery.py
import asyncio
from enum import Enum
from datetime import datetime, timedelta
from typing import Dict, List, Callable

class FailureType(Enum):
    DATABASE_CONNECTION = "database_connection"
    REDIS_CONNECTION = "redis_connection"
    EXTERNAL_SERVICE = "external_service"
    HIGH_MEMORY_USAGE = "high_memory_usage"
    HIGH_CPU_USAGE = "high_cpu_usage"

class AutoRecoveryManager:
    def __init__(self):
        self.recovery_actions = {
            FailureType.DATABASE_CONNECTION: [
                self._restart_db_connections,
                self._switch_to_backup_db,
                self._enable_maintenance_mode
            ],
            FailureType.REDIS_CONNECTION: [
                self._restart_redis_connection,
                self._switch_to_backup_redis,
                self._disable_caching
            ],
            FailureType.HIGH_MEMORY_USAGE: [
                self._trigger_garbage_collection,
                self._restart_worker_processes,
                self._scale_out_instances
            ]
        }

        self.failure_history = []
        self.recovery_in_progress = set()

    async def handle_failure(self, failure_type: FailureType, context: Dict):
        """éšœå®³ã®è‡ªå‹•å¾©æ—§å‡¦ç†"""
        if failure_type in self.recovery_in_progress:
            logger.info(f"å¾©æ—§å‡¦ç†å®Ÿè¡Œä¸­ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {failure_type.value}")
            return

        self.recovery_in_progress.add(failure_type)

        try:
            # éšœå®³å±¥æ­´ã‚’è¨˜éŒ²
            failure_record = {
                'type': failure_type.value,
                'timestamp': datetime.utcnow(),
                'context': context,
                'recovery_attempts': []
            }

            logger.error("éšœå®³æ¤œçŸ¥ã€è‡ªå‹•å¾©æ—§é–‹å§‹",
                failure_type=failure_type.value,
                context=context,
                action="auto_recovery_start"
            )

            # å¾©æ—§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é †æ¬¡å®Ÿè¡Œ
            actions = self.recovery_actions.get(failure_type, [])

            for i, action in enumerate(actions):
                try:
                    logger.info(f"å¾©æ—§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ: {action.__name__}",
                        failure_type=failure_type.value,
                        action_index=i + 1,
                        total_actions=len(actions)
                    )

                    result = await action(context)

                    failure_record['recovery_attempts'].append({
                        'action': action.__name__,
                        'result': 'success',
                        'timestamp': datetime.utcnow()
                    })

                    # å¾©æ—§ç¢ºèª
                    if await self._verify_recovery(failure_type):
                        logger.info("è‡ªå‹•å¾©æ—§æˆåŠŸ",
                            failure_type=failure_type.value,
                            successful_action=action.__name__,
                            action="auto_recovery_success"
                        )
                        break

                except Exception as e:
                    failure_record['recovery_attempts'].append({
                        'action': action.__name__,
                        'result': 'failed',
                        'error': str(e),
                        'timestamp': datetime.utcnow()
                    })

                    logger.error(f"å¾©æ—§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¤±æ•—: {action.__name__}",
                        error=e,
                        failure_type=failure_type.value
                    )

                    # æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¾ã§å¾…æ©Ÿ
                    await asyncio.sleep(30)

            else:
                # å…¨ã¦ã®å¾©æ—§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå¤±æ•—
                logger.error("è‡ªå‹•å¾©æ—§å¤±æ•—ã€æ‰‹å‹•å¯¾å¿œãŒå¿…è¦",
                    failure_type=failure_type.value,
                    action="auto_recovery_failed"
                )

                await self._escalate_to_human(failure_type, failure_record)

            self.failure_history.append(failure_record)

        finally:
            self.recovery_in_progress.discard(failure_type)

    async def _restart_db_connections(self, context: Dict) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®å†èµ·å‹•"""
        try:
            # æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’ã‚¯ãƒªã‚¢
            await db_cluster.primary_engine.dispose()
            for replica_engine in db_cluster.replica_engines:
                await replica_engine.dispose()

            # æ–°ã—ã„æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ
            await db_cluster._initialize_engines()

            return True
        except Exception as e:
            logger.error("DBæ¥ç¶šå†èµ·å‹•å¤±æ•—", error=e)
            return False

    async def _switch_to_backup_db(self, context: Dict) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—DBã¸ã®åˆ‡ã‚Šæ›¿ãˆ"""
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—DBã®å¥å…¨æ€§ç¢ºèª
            backup_engine = create_async_engine(
                os.getenv('BACKUP_DATABASE_URL')
            )

            async with backup_engine.connect() as conn:
                await conn.execute(text("SELECT 1"))

            # ãƒ—ãƒ©ã‚¤ãƒãƒªDBã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«åˆ‡ã‚Šæ›¿ãˆ
            db_cluster.primary_engine = backup_engine

            logger.info("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—DBã«åˆ‡ã‚Šæ›¿ãˆå®Œäº†",
                action="database_failover"
            )

            return True
        except Exception as e:
            logger.error("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—DBåˆ‡ã‚Šæ›¿ãˆå¤±æ•—", error=e)
            return False

    async def _verify_recovery(self, failure_type: FailureType) -> bool:
        """å¾©æ—§ç¢ºèª"""
        try:
            if failure_type == FailureType.DATABASE_CONNECTION:
                async with await db_cluster.get_write_session() as session:
                    await session.execute(text("SELECT 1"))
                return True

            elif failure_type == FailureType.REDIS_CONNECTION:
                await redis_client.ping()
                return True

            # ãã®ä»–ã®ç¢ºèªå‡¦ç†...

        except Exception:
            return False

        return False

    async def _escalate_to_human(self, failure_type: FailureType, failure_record: Dict):
        """äººé–“ã¸ã®ç·Šæ€¥ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        escalation_data = {
            'severity': 'CRITICAL',
            'failure_type': failure_type.value,
            'auto_recovery_failed': True,
            'failure_record': failure_record,
            'required_action': 'IMMEDIATE_MANUAL_INTERVENTION'
        }

        # ç·Šæ€¥é€šçŸ¥ã®é€ä¿¡
        await self._send_emergency_notification(escalation_data)

        # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰ã®æœ‰åŠ¹åŒ–
        await self._enable_maintenance_mode()
```

---

## ğŸ”„ ç½å®³å¾©æ—§ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

#### 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```python
# utils/backup_manager.py
import asyncio
import boto3
from datetime import datetime, timedelta
from typing import List, Dict

class BackupManager:
    def __init__(self):
        self.s3_client = boto3.client('s3')
        self.backup_bucket = os.getenv('BACKUP_S3_BUCKET')
        self.retention_policy = {
            'daily': 30,    # 30æ—¥é–“
            'weekly': 12,   # 12é€±é–“
            'monthly': 12   # 12ãƒ¶æœˆé–“
        }

    async def create_database_backup(self, backup_type: str = 'daily') -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        backup_filename = f"db_backup_{backup_type}_{timestamp}.sql"

        try:
            # pg_dumpã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_command = [
                'pg_dump',
                '--host', os.getenv('DB_HOST'),
                '--port', os.getenv('DB_PORT', '5432'),
                '--username', os.getenv('DB_USER'),
                '--dbname', os.getenv('DB_NAME'),
                '--verbose',
                '--clean',
                '--no-owner',
                '--no-privileges',
                '--format=custom'
            ]

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            process = await asyncio.create_subprocess_exec(
                *backup_command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                s3_key = f"database/{backup_type}/{backup_filename}"

                self.s3_client.put_object(
                    Bucket=self.backup_bucket,
                    Key=s3_key,
                    Body=stdout,
                    StorageClass='STANDARD_IA',  # ã‚³ã‚¹ãƒˆæœ€é©åŒ–
                    ServerSideEncryption='AES256'
                )

                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
                await self._record_backup_metadata({
                    'filename': backup_filename,
                    's3_key': s3_key,
                    'backup_type': backup_type,
                    'timestamp': datetime.utcnow(),
                    'size_bytes': len(stdout),
                    'status': 'completed'
                })

                logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†",
                    backup_type=backup_type,
                    filename=backup_filename,
                    size_mb=round(len(stdout) / 1024 / 1024, 2),
                    action="backup_completed"
                )

                return s3_key

            else:
                raise Exception(f"pg_dump failed: {stderr.decode()}")

        except Exception as e:
            logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—",
                error=e,
                backup_type=backup_type,
                action="backup_failed"
            )
            raise

    async def restore_database(self, backup_s3_key: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å¾©å…ƒ"""
        try:
            # S3ã‹ã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            response = self.s3_client.get_object(
                Bucket=self.backup_bucket,
                Key=backup_s3_key
            )
            backup_data = response['Body'].read()

            # å¾©å…ƒå‰ã®ç¢ºèª
            confirmation = await self._confirm_restoration()
            if not confirmation:
                return False

            # pg_restoreã‚’ä½¿ç”¨ã—ã¦å¾©å…ƒ
            restore_command = [
                'pg_restore',
                '--host', os.getenv('DB_HOST'),
                '--port', os.getenv('DB_PORT', '5432'),
                '--username', os.getenv('DB_USER'),
                '--dbname', os.getenv('DB_NAME'),
                '--verbose',
                '--clean',
                '--if-exists'
            ]

            process = await asyncio.create_subprocess_exec(
                *restore_command,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate(input=backup_data)

            if process.returncode == 0:
                logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©å…ƒå®Œäº†",
                    backup_s3_key=backup_s3_key,
                    action="restore_completed"
                )
                return True
            else:
                raise Exception(f"pg_restore failed: {stderr.decode()}")

        except Exception as e:
            logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©å…ƒå¤±æ•—",
                error=e,
                backup_s3_key=backup_s3_key,
                action="restore_failed"
            )
            return False

    async def cleanup_old_backups(self):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤"""
        for backup_type, retention_days in self.retention_policy.items():
            cutoff_date = datetime.utcnow() - timedelta(days=retention_days)

            # S3ã‹ã‚‰å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            response = self.s3_client.list_objects_v2(
                Bucket=self.backup_bucket,
                Prefix=f"database/{backup_type}/"
            )

            deleted_count = 0
            for obj in response.get('Contents', []):
                if obj['LastModified'].replace(tzinfo=None) < cutoff_date:
                    self.s3_client.delete_object(
                        Bucket=self.backup_bucket,
                        Key=obj['Key']
                    )
                    deleted_count += 1

            logger.info("å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤å®Œäº†",
                backup_type=backup_type,
                deleted_count=deleted_count,
                retention_days=retention_days,
                action="backup_cleanup"
            )

# å®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
async def schedule_backups():
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°"""
    backup_manager = BackupManager()

    while True:
        now = datetime.utcnow()

        # æ¯æ—¥åˆå‰2æ™‚ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if now.hour == 2 and now.minute == 0:
            await backup_manager.create_database_backup('daily')

            # æ—¥æ›œæ—¥ã¯é€±æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚‚ä½œæˆ
            if now.weekday() == 6:
                await backup_manager.create_database_backup('weekly')

            # æœˆåˆã¯æœˆæ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚‚ä½œæˆ
            if now.day == 1:
                await backup_manager.create_database_backup('monthly')

            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤
            await backup_manager.cleanup_old_backups()

        # 1åˆ†é–“å¾…æ©Ÿ
        await asyncio.sleep(60)
```

#### 2. ç½å®³å¾©æ—§è¨ˆç”»

```python
# utils/disaster_recovery.py
class DisasterRecoveryManager:
    def __init__(self):
        self.recovery_procedures = {
            'data_center_failure': self._handle_data_center_failure,
            'database_corruption': self._handle_database_corruption,
            'complete_system_failure': self._handle_complete_system_failure,
            'cyber_attack': self._handle_cyber_attack_recovery
        }

        self.rto_targets = {  # Recovery Time Objective
            'critical': timedelta(hours=1),
            'high': timedelta(hours=4),
            'medium': timedelta(hours=8),
            'low': timedelta(hours=24)
        }

        self.rpo_targets = {  # Recovery Point Objective
            'critical': timedelta(minutes=15),
            'high': timedelta(hours=1),
            'medium': timedelta(hours=4),
            'low': timedelta(hours=8)
        }

    async def initiate_disaster_recovery(
        self,
        disaster_type: str,
        severity: str,
        affected_components: List[str]
    ):
        """ç½å®³å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®é–‹å§‹"""
        recovery_id = f"DR-{int(datetime.utcnow().timestamp())}"

        recovery_plan = {
            'recovery_id': recovery_id,
            'disaster_type': disaster_type,
            'severity': severity,
            'affected_components': affected_components,
            'start_time': datetime.utcnow(),
            'rto_target': self.rto_targets.get(severity),
            'rpo_target': self.rpo_targets.get(severity),
            'status': 'in_progress',
            'steps_completed': []
        }

        logger.critical("ç½å®³å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹",
            recovery_id=recovery_id,
            disaster_type=disaster_type,
            severity=severity,
            affected_components=affected_components,
            action="disaster_recovery_start"
        )

        try:
            # ç½å®³ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå¾©æ—§æ‰‹é †ã‚’å®Ÿè¡Œ
            if disaster_type in self.recovery_procedures:
                await self.recovery_procedures[disaster_type](recovery_plan)
            else:
                await self._generic_recovery_procedure(recovery_plan)

            recovery_plan['status'] = 'completed'
            recovery_plan['end_time'] = datetime.utc
```
